{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10009580,"sourceType":"datasetVersion","datasetId":6162105},{"sourceId":10014546,"sourceType":"datasetVersion","datasetId":6165612},{"sourceId":209302822,"sourceType":"kernelVersion"},{"sourceId":209507915,"sourceType":"kernelVersion"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T03:16:55.134478Z","iopub.execute_input":"2024-11-26T03:16:55.135290Z","iopub.status.idle":"2024-11-26T03:16:55.139675Z","shell.execute_reply.started":"2024-11-26T03:16:55.135248Z","shell.execute_reply":"2024-11-26T03:16:55.138610Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"#Zero_shot_prompt_dev_evaluate\n\nwith open('/kaggle/input/mrc-evaluate-close-sourced-chatgpt/zero_shot_prompt_dev_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:20:58.671507Z","iopub.execute_input":"2024-11-24T13:20:58.671881Z","iopub.status.idle":"2024-11-24T13:20:58.805814Z","shell.execute_reply.started":"2024-11-24T13:20:58.671849Z","shell.execute_reply":"2024-11-24T13:20:58.804743Z"}},"outputs":[{"name":"stdout","text":"EM Score: 0.5838074398249453\nF1 Score: 0.8290632634503127\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"#One_shot_prompt_dev_evaluate\n\nwith open('/kaggle/input/mrc-evaluate-close-sourced-chatgpt/one_shot_prompt_dev_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:23:13.991028Z","iopub.execute_input":"2024-11-24T13:23:13.992131Z","iopub.status.idle":"2024-11-24T13:23:14.041879Z","shell.execute_reply.started":"2024-11-24T13:23:13.992086Z","shell.execute_reply":"2024-11-24T13:23:14.040962Z"}},"outputs":[{"name":"stdout","text":"EM Score: 0.5912472647702407\nF1 Score: 0.8401426671310047\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"#few_shot_prompt_dev_evaluate\n\nwith open('/kaggle/input/mrc-evaluate-close-sourced-chatgpt/few_shot_prompt_dev_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:23:22.168142Z","iopub.execute_input":"2024-11-24T13:23:22.168502Z","iopub.status.idle":"2024-11-24T13:23:22.239712Z","shell.execute_reply.started":"2024-11-24T13:23:22.168468Z","shell.execute_reply":"2024-11-24T13:23:22.238657Z"}},"outputs":[{"name":"stdout","text":"EM Score: 0.5886214442013129\nF1 Score: 0.8414061217673214\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"#chain_of_thought_prompt_dev_evaluate\n\nwith open('/kaggle/input/mrc-evaluate-close-sourced-chatgpt/chain_of_thought_prompt_dev_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:23:46.597603Z","iopub.execute_input":"2024-11-24T13:23:46.598392Z","iopub.status.idle":"2024-11-24T13:23:46.668567Z","shell.execute_reply.started":"2024-11-24T13:23:46.598352Z","shell.execute_reply":"2024-11-24T13:23:46.667533Z"}},"outputs":[{"name":"stdout","text":"EM Score: 0.5811816192560175\nF1 Score: 0.8480970142312126\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"#Zero_shot_prompt_test_evaluate\n\nwith open('/kaggle/input/mrc-evaluate-close-sourced-chatgpt/zero_shot_prompt_test_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:25:41.038544Z","iopub.execute_input":"2024-11-24T13:25:41.039180Z","iopub.status.idle":"2024-11-24T13:25:41.135013Z","shell.execute_reply.started":"2024-11-24T13:25:41.039139Z","shell.execute_reply":"2024-11-24T13:25:41.133889Z"}},"outputs":[{"name":"stdout","text":"EM Score: 0.5773755656108597\nF1 Score: 0.8330293663478862\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"#one_shot_prompt_test_evaluate\n\nwith open('/kaggle/input/mrc-evaluate-close-sourced-chatgpt/one_shot_prompt_test_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T13:26:19.853379Z","iopub.execute_input":"2024-11-25T13:26:19.853893Z","iopub.status.idle":"2024-11-25T13:26:19.942777Z","shell.execute_reply.started":"2024-11-25T13:26:19.853842Z","shell.execute_reply":"2024-11-25T13:26:19.941576Z"}},"outputs":[{"name":"stdout","text":"EM Score: 0.5904977375565611\nF1 Score: 0.8333299041062838\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"#few_shot_prompt_test_evaluate\n\nwith open('/kaggle/input/few-shot-prompt-test-evaluate/few_shot_prompt_test_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T13:29:47.056795Z","iopub.execute_input":"2024-11-25T13:29:47.057913Z","iopub.status.idle":"2024-11-25T13:29:47.155446Z","shell.execute_reply.started":"2024-11-25T13:29:47.057868Z","shell.execute_reply":"2024-11-25T13:29:47.154235Z"}},"outputs":[{"name":"stdout","text":"EM Score: 0.5751131221719457\nF1 Score: 0.8358137600144059\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"#chain_of_thought_prompt_test_evaluate\n\nwith open('/kaggle/input/few-shot-prompt-test-evaluate/chain_of_thought_prompt_test_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T13:30:33.045388Z","iopub.execute_input":"2024-11-25T13:30:33.045793Z","iopub.status.idle":"2024-11-25T13:30:33.130871Z","shell.execute_reply.started":"2024-11-25T13:30:33.045757Z","shell.execute_reply":"2024-11-25T13:30:33.129665Z"}},"outputs":[{"name":"stdout","text":"EM Score: 0.5882352941176471\nF1 Score: 0.847799740117859\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"#zero_shot_prompt_dev_evaluate\n\nwith open('/kaggle/input/mrc-evaluate-close-sourced-gemini/zero_shot_prompt_dev_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:26:04.561812Z","iopub.execute_input":"2024-11-24T13:26:04.562240Z","iopub.status.idle":"2024-11-24T13:26:04.651959Z","shell.execute_reply.started":"2024-11-24T13:26:04.562206Z","shell.execute_reply":"2024-11-24T13:26:04.650997Z"}},"outputs":[{"name":"stdout","text":"EM Score: 0.638074398249453\nF1 Score: 0.8606686681574911\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"#one_shot_prompt_dev_evaluate\n\nwith open('/kaggle/input/mrc-evaluate-close-sourced-gemini/one_shot_prompt_dev_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:26:18.781218Z","iopub.execute_input":"2024-11-24T13:26:18.781551Z","iopub.status.idle":"2024-11-24T13:26:18.938555Z","shell.execute_reply.started":"2024-11-24T13:26:18.781522Z","shell.execute_reply":"2024-11-24T13:26:18.937354Z"}},"outputs":[{"name":"stdout","text":"EM Score: 0.6582056892778994\nF1 Score: 0.8707789324138233\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"#few_shot_prompt_dev_evaluate\n\nwith open('/kaggle/input/mrc-evaluate-close-sourced-gemini/few_shot_prompt_dev_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:26:24.917369Z","iopub.execute_input":"2024-11-24T13:26:24.918323Z","iopub.status.idle":"2024-11-24T13:26:24.998558Z","shell.execute_reply.started":"2024-11-24T13:26:24.918282Z","shell.execute_reply":"2024-11-24T13:26:24.997434Z"}},"outputs":[{"name":"stdout","text":"EM Score: 0.6463894967177243\nF1 Score: 0.876269203536275\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"#chain_of_thought_prompt_dev_evaluate\n\nwith open('/kaggle/input/mrc-evaluate-close-sourced-gemini/chain_of_thought_prompt_dev_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:26:39.471458Z","iopub.execute_input":"2024-11-24T13:26:39.471822Z","iopub.status.idle":"2024-11-24T13:26:39.541714Z","shell.execute_reply.started":"2024-11-24T13:26:39.471788Z","shell.execute_reply":"2024-11-24T13:26:39.540679Z"}},"outputs":[{"name":"stdout","text":"EM Score: 0.7041575492341356\nF1 Score: 0.8962752460022315\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"#zero_shot_prompt_test_evaluate\n\nwith open('/kaggle/input/mrc-evaluate-close-sourced-gemini/zero_shot_prompt_test_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:26:59.506072Z","iopub.execute_input":"2024-11-24T13:26:59.506430Z","iopub.status.idle":"2024-11-24T13:26:59.581367Z","shell.execute_reply.started":"2024-11-24T13:26:59.506398Z","shell.execute_reply":"2024-11-24T13:26:59.580104Z"}},"outputs":[{"name":"stdout","text":"EM Score: 0.6298642533936651\nF1 Score: 0.8577758224755521\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"#one_shot_prompt_test_evaluate\n\nwith open('/kaggle/input/mrc-evaluate-close-sourced-gemini/one_shot_prompt_test_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:27:06.856159Z","iopub.execute_input":"2024-11-24T13:27:06.856982Z","iopub.status.idle":"2024-11-24T13:27:06.929060Z","shell.execute_reply.started":"2024-11-24T13:27:06.856941Z","shell.execute_reply":"2024-11-24T13:27:06.927958Z"}},"outputs":[{"name":"stdout","text":"EM Score: 0.6438914027149322\nF1 Score: 0.8650047178798353\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"#few_shot_prompt_test_evaluate\n\nwith open('/kaggle/input/mrc-evaluate-close-sourced-gemini/few_shot_prompt_test_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:27:11.457484Z","iopub.execute_input":"2024-11-24T13:27:11.458440Z","iopub.status.idle":"2024-11-24T13:27:11.532329Z","shell.execute_reply.started":"2024-11-24T13:27:11.458395Z","shell.execute_reply":"2024-11-24T13:27:11.531270Z"}},"outputs":[{"name":"stdout","text":"EM Score: 0.6289592760180995\nF1 Score: 0.8684270310076388\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"#chain_of_thought_prompt_test_evaluate\n\nwith open('/kaggle/input/mrc-evaluate-close-sourced-gemini/chain_of_thought_prompt_test_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T13:27:23.097062Z","iopub.execute_input":"2024-11-24T13:27:23.097422Z","iopub.status.idle":"2024-11-24T13:27:23.176095Z","shell.execute_reply.started":"2024-11-24T13:27:23.097389Z","shell.execute_reply":"2024-11-24T13:27:23.174945Z"}},"outputs":[{"name":"stdout","text":"EM Score: 0.6914027149321267\nF1 Score: 0.8937912473837032\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"#zero_shot_prompt_test_evaluate\n\nwith open('/kaggle/input/llama-test-eva/zero_shot_prompt_test_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n#one_shot_prompt_test_evaluate\n\nwith open('/kaggle/input/llama-test-eva/one_shot_prompt_test_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n#few_shot_prompt_test_evaluate\n\nwith open('/kaggle/input/llama-test-eva/few_shot_prompt_test_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n#chain_of_thought_prompt_test_evaluate\n\nwith open('/kaggle/input/llama-test-eva/chain_of_thought_prompt_test_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T03:18:48.435703Z","iopub.execute_input":"2024-11-26T03:18:48.436158Z","iopub.status.idle":"2024-11-26T03:18:48.797460Z","shell.execute_reply.started":"2024-11-26T03:18:48.436120Z","shell.execute_reply":"2024-11-26T03:18:48.796278Z"}},"outputs":[{"name":"stdout","text":"EM Score: 0.4601809954751131\nF1 Score: 0.7345283498650244\nEM Score: 0.4461538461538462\nF1 Score: 0.7146933417308702\nEM Score: 0.40226244343891404\nF1 Score: 0.686241657951131\nEM Score: 0.35429864253393667\nF1 Score: 0.6693653711149615\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"#zero_shot_prompt_dev_evaluate\nwith open('/kaggle/input/llama-test-eva/zero_shot_prompt_dev_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n#one_shot_prompt_dev_evaluate\n\nwith open('/kaggle/input/llama-test-eva/one_shot_prompt_dev_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n#few_shot_prompt_dev_evaluate\n\nwith open('/kaggle/input/llama-test-eva/few_shot_prompt_dev_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n#chain_of_thought_prompt_dev_evaluate\n\nwith open('/kaggle/input/llama-test-eva/chain_of_thought_prompt_dev_evaluate.json', 'r') as file:\n    data_dev = json.load(file)\n\naverage_em_score = []\naverage_f1_score = []\n\nfor _, data in enumerate(data_dev[\"data\"]):\n    for _, paragraph in enumerate(data[\"paragraphs\"]):\n        for _, qa in (enumerate(paragraph[\"qas\"])):\n            average_em_score.append(qa['em_score'])\n            average_f1_score.append(qa['f1_score'])\n            \naverage_em_score = sum(average_em_score) / len(average_em_score)\naverage_f1_score = sum(average_f1_score) / len(average_f1_score)\nprint(f\"EM Score: {average_em_score}\")\nprint(f\"F1 Score: {average_f1_score}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T03:16:58.869222Z","iopub.execute_input":"2024-11-26T03:16:58.869588Z","iopub.status.idle":"2024-11-26T03:16:59.255155Z","shell.execute_reply.started":"2024-11-26T03:16:58.869553Z","shell.execute_reply":"2024-11-26T03:16:59.253952Z"}},"outputs":[{"name":"stdout","text":"EM Score: 0.4652078774617068\nF1 Score: 0.7460107250414266\nEM Score: 0.43588621444201314\nF1 Score: 0.712093305136324\nEM Score: 0.41444201312910284\nF1 Score: 0.6875750088209479\nEM Score: 0.3304157549234136\nF1 Score: 0.663444306136755\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}